{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5cb84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import CoronagraphEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99383d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"Coronagraph-v0\",\n",
    "    entry_point=\"your_module:CoronagraphEnvironment\",  # Replace with correct module path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e55738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing coronagraph env. might take a minute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image': OldStyleField([0., 0., 0., ..., 0., 0., 0.], shape=(57600,), dtype=float32),\n",
       " 'slopes': OldStyleField([[-9.1864536e-07,  9.1164829e-06, -4.1101016e-06, ...,\n",
       "                  4.1101016e-06, -9.1164829e-06,  9.1864536e-07],\n",
       "                [-9.1864536e-07,  1.2017832e-05,  1.7143098e-05, ...,\n",
       "                 -1.7143098e-05, -1.2017832e-05,  9.1864536e-07]],\n",
       "               shape=(2, 1849), dtype=float32),\n",
       " 'strehl': array([1.], dtype=float32)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = CoronagraphEnvironment(num_modes = 4)\n",
    "e._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06749603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing coronagraph env. might take a minute.\n",
      "Logging to ./logs/\n",
      "Using cuda device\n",
      "initializing coronagraph env. might take a minute.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000264 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 601      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.00027    |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3          |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1198       |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00468318 |\n",
      "|    clip_fraction        | 0.0158     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.66      |\n",
      "|    explained_variance   | -32.3      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00518   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    std                  | 0.992      |\n",
      "|    value_loss           | 1.91e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 2.80 +/- 3.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 2.8         |\n",
      "|    mean_reward          | 0.000755    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004876974 |\n",
      "|    clip_fraction        | 0.0224      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.63       |\n",
      "|    explained_variance   | -0.0299     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 2.42e-09    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000269 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 1800     |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000265     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 2396         |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017000512 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | -0.00851     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00237      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00069     |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 2.63e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 0.000257    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005166376 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.66       |\n",
      "|    explained_variance   | 0.0132      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00701     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 2.98e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000269 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 3000     |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000271     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3598         |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022462092 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.67        |\n",
      "|    explained_variance   | 0.00225      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00257     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000881    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 4.52e-09     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.00026      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4193         |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025881743 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.65        |\n",
      "|    explained_variance   | 0.00885      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00761      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000932    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 5.41e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 0.000253     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038497364 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | 0.00607      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0126       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 6.23e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000269 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 4804     |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000257     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 5406         |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044362783 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.64        |\n",
      "|    explained_variance   | -0.000522    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0141      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 1.07e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 0.000246    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004565305 |\n",
      "|    clip_fraction        | 0.0196      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.64       |\n",
      "|    explained_variance   | -0.0166     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00672     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 9.92e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000263 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 6009     |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.000262    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 6610        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005251671 |\n",
      "|    clip_fraction        | 0.0157      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.61       |\n",
      "|    explained_variance   | -0.0282     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00534    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 1.21e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000267     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 7207         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040678587 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.6         |\n",
      "|    explained_variance   | 0.0038       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00421     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 7.7e-09      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 1          |\n",
      "|    mean_reward          | 0.000252   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00394071 |\n",
      "|    clip_fraction        | 0.0178     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -5.6       |\n",
      "|    explained_variance   | -0.0049    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00715   |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00285   |\n",
      "|    std                  | 0.981      |\n",
      "|    value_loss           | 1.31e-08   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000268 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 7808     |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.00026      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 8412         |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027148998 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.6         |\n",
      "|    explained_variance   | -0.00141     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00447     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 1.05e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 0.000263    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005830232 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | 0.00452     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 2.76e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000262 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 9013     |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000261     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 9614         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049545784 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.58        |\n",
      "|    explained_variance   | 0.00732      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0217      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 6.38e-09     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000257     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 10217        |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034556238 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.59        |\n",
      "|    explained_variance   | -0.0425      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00285      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 4.73e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 0.000296     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055457633 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.58        |\n",
      "|    explained_variance   | -0.0552      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000238    |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00382     |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 9.09e-09     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000258 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 10813    |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.000264    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 11408       |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006511163 |\n",
      "|    clip_fraction        | 0.0559      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.59       |\n",
      "|    explained_variance   | -0.000217   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00729    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00639    |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 1.06e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 0.000253    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004365796 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.61       |\n",
      "|    explained_variance   | -0.00308    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00283    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 4.03e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000267 |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 12012    |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000262     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 12616        |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043980647 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | -0.0443      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0103      |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 5.74e-09     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 0.000279    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004688684 |\n",
      "|    clip_fraction        | 0.0247      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.65       |\n",
      "|    explained_variance   | 0.00223     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.012       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 2.38e-08    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.00027  |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 13211    |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000258     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3            |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 13809        |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069371983 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.65        |\n",
      "|    explained_variance   | -0.0352      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0198      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 4.34e-09     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.000271    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 14407       |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004418191 |\n",
      "|    clip_fraction        | 0.0159      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | -0.00573    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00231     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00299    |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 6.77e-09    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 1            |\n",
      "|    mean_reward          | 0.000261     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028171614 |\n",
      "|    clip_fraction        | 0.00518      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.63        |\n",
      "|    explained_variance   | 0.00377      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00138      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00222     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 1.3e-08      |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.00027  |\n",
      "| time/              |          |\n",
      "|    fps             | 3        |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 15009    |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.000272     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1            |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 43190        |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038585612 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.64        |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0126      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 1.04e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 1           |\n",
      "|    mean_reward          | 0.000255    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005204092 |\n",
      "|    clip_fraction        | 0.0285      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -5.62       |\n",
      "|    explained_variance   | -0.0904     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0235     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 7.39e-09    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0.000271 |\n",
      "| time/              |          |\n",
      "|    fps             | 1        |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 43818    |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.00027      |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1            |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 44482        |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064162994 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -5.58        |\n",
      "|    explained_variance   | 0.00322      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.013       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00412     |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 2.98e-08     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     34\u001b[39m eval_callback = EvalCallback(\n\u001b[32m     35\u001b[39m     eval_env,\n\u001b[32m     36\u001b[39m     best_model_save_path=\u001b[33m\"\u001b[39m\u001b[33m./models/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     render=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[32m     47\u001b[39m model.save(\u001b[33m\"\u001b[39m\u001b[33mppo_coronagraph_final\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:324\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_timesteps < total_timesteps:\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     continue_training = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[39m, in \u001b[36mOnPolicyAlgorithm.collect_rollouts\u001b[39m\u001b[34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[39m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[32m    215\u001b[39m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[32m    216\u001b[39m         clipped_actions = np.clip(actions, \u001b[38;5;28mself\u001b[39m.action_space.low, \u001b[38;5;28mself\u001b[39m.action_space.high)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m new_obs, rewards, dones, infos = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;28mself\u001b[39m.num_timesteps += env.num_envs\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[39m, in \u001b[36mMonitor.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.needs_reset:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTried to step environment that needs reset\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.rewards.append(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\RL\\environment.py:189\u001b[39m, in \u001b[36mCoronagraphEnvironment.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mself\u001b[39m.set_dm(action=action)\n\u001b[32m    187\u001b[39m \u001b[38;5;28mself\u001b[39m.iteration_counter -= \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m reward = CoronagraphEnvironment.reward_function(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_strehl_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    191\u001b[39m terminated = \u001b[38;5;28mself\u001b[39m.iteration_counter <= \u001b[32m0\u001b[39m\n\u001b[32m    192\u001b[39m truncated = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\RL\\environment.py:129\u001b[39m, in \u001b[36mCoronagraphEnvironment.get_strehl_ratio\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_strehl_ratio\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    128\u001b[39m     wf_aberrated = \u001b[38;5;28mself\u001b[39m.deformable_mirror(\u001b[38;5;28mself\u001b[39m.wf)\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     psf_aberrated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwf_aberrated\u001b[49m\u001b[43m)\u001b[49m.intensity\n\u001b[32m    130\u001b[39m     peak_aberrated = np.max(psf_aberrated)\n\u001b[32m    132\u001b[39m     psf_ideal = \u001b[38;5;28mself\u001b[39m.prop(\u001b[38;5;28mself\u001b[39m.wf).intensity\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\hcipy\\optics\\optical_element.py:28\u001b[39m, in \u001b[36mOpticalElement.__call__\u001b[39m\u001b[34m(self, wavefront)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, wavefront):\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Propagate a wavefront forward through the optical element.\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[33;03m        The propagated wavefront.\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavefront\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\hcipy\\optics\\optical_element.py:758\u001b[39m, in \u001b[36mmake_agnostic_forward.<locals>.res\u001b[39m\u001b[34m(self, wavefront, *args, **kwargs)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mres\u001b[39m(\u001b[38;5;28mself\u001b[39m, wavefront, *args, **kwargs):\n\u001b[32m    755\u001b[39m     \u001b[38;5;66;03m# Look up instance data\u001b[39;00m\n\u001b[32m    756\u001b[39m     instance_data = \u001b[38;5;28mself\u001b[39m.get_instance_data(wavefront.grid, \u001b[38;5;28;01mNone\u001b[39;00m, wavefront.wavelength)\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwavefront\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\hcipy\\propagation\\fraunhofer.py:70\u001b[39m, in \u001b[36mFraunhoferPropagator.forward\u001b[39m\u001b[34m(self, instance_data, wavefront)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;129m@make_agnostic_forward\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, instance_data, wavefront):\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Propagate a wavefront forward through the lens.\u001b[39;00m\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m        The wavefront after the propagation.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     U_new = \u001b[43minstance_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfourier_transform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwavefront\u001b[49m\u001b[43m.\u001b[49m\u001b[43melectric_field\u001b[49m\u001b[43m)\u001b[49m * instance_data.norm_factor\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Wavefront(Field(U_new, instance_data.output_grid), wavefront.wavelength, wavefront.input_stokes_vector)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\hcipy\\fourier\\fourier_transform.py:221\u001b[39m, in \u001b[36mmultiplex_for_tensor_fields.<locals>.inner\u001b[39m\u001b[34m(self, field)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(\u001b[38;5;28mself\u001b[39m, field):\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m field.is_scalar_field:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    223\u001b[39m         f = field.reshape((-\u001b[32m1\u001b[39m, field.grid.size))\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\hcipy\\fourier\\fast_fourier_transform.py:331\u001b[39m, in \u001b[36mFastFourierTransform.forward\u001b[39m\u001b[34m(self, field)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.emulate_fftshifts:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28mself\u001b[39m.internal_array = np.fft.ifftshift(\u001b[38;5;28mself\u001b[39m.internal_array)\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m fft_array = \u001b[43m_fft_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfftn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minternal_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.emulate_fftshifts:\n\u001b[32m    334\u001b[39m     fft_array = np.fft.fftshift(fft_array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\hcipy\\_math\\fft.py:62\u001b[39m, in \u001b[36m_make_func.<locals>.func\u001b[39m\u001b[34m(x, overwrite_x, method, threads, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pyfftw_func(x, *args, **kwargs, workers=threads, overwrite_x=overwrite_x)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mscipy\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscipy_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Numpy doesn't always retain the correct bit depth. Cast the results to the correct bit depth here.\u001b[39;00m\n\u001b[32m     65\u001b[39m     dtype_in = x.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[39m, in \u001b[36m_ScipyBackend.__ua_function__\u001b[39m\u001b[34m(method, args, kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:115\u001b[39m, in \u001b[36mfftn\u001b[39m\u001b[34m(x, s, axes, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfftn\u001b[39m(x, s=\u001b[38;5;28;01mNone\u001b[39;00m, axes=\u001b[38;5;28;01mNone\u001b[39;00m, norm=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    114\u001b[39m          overwrite_x=\u001b[38;5;28;01mFalse\u001b[39;00m, workers=\u001b[38;5;28;01mNone\u001b[39;00m, *, plan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_nD\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfftn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfftn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:57\u001b[39m, in \u001b[36m_execute_nD\u001b[39m\u001b[34m(func_str, pocketfft_func, x, s, axes, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[32m     56\u001b[39m     x = np.asarray(x)\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m norm = _validate_fft_args(workers, plan, norm)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[33m'\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Projects\\DM + RL\\.venv\\Lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:149\u001b[39m, in \u001b[36mc2cn\u001b[39m\u001b[34m(forward, x, s, axes, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m    146\u001b[39m norm = _normalization(norm, forward)\n\u001b[32m    147\u001b[39m out = (tmp \u001b[38;5;28;01mif\u001b[39;00m overwrite_x \u001b[38;5;129;01mand\u001b[39;00m tmp.dtype.kind == \u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from environment import CoronagraphEnvironment  # Or import your env class directly\n",
    "\n",
    "# Wrap in a function so it works cleanly\n",
    "def make_env():\n",
    "    return Monitor(CoronagraphEnvironment(num_modes=4))\n",
    "\n",
    "# Vectorized environment (Stable-Baselines3 requires this)\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "# Set up logging\n",
    "log_path = \"./logs/\"\n",
    "new_logger = configure(log_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "# Instantiate the PPO agent with a policy that handles Dict spaces\n",
    "model = PPO(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_path,\n",
    ")\n",
    "\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Optional: EvalCallback to evaluate the agent during training\n",
    "eval_env = DummyVecEnv([make_env])\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./models/\",\n",
    "    log_path=\"./logs/\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=100_000, callback=eval_callback)\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"ppo_coronagraph_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "env = CoronagraphEnvironment()\n",
    "model = PPO.load(\"ppo_coronagraph_final\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3ba59",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DummyVecEnv' object has no attribute '_get_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43meval_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_obs\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'DummyVecEnv' object has no attribute '_get_obs'"
     ]
    }
   ],
   "source": [
    "eval_env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892498c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
